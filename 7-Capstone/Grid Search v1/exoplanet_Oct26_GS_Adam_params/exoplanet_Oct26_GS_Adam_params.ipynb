{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "\n",
    "## Capstone project: Exoplanet search (from Kaggle)\n",
    "\n",
    "Student Name: Graciano Patino\n",
    "\n",
    "Kaggle reference: https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data?/\n",
    "\n",
    "The mission as stated in the Github (https://github.com/winterdelta/KeplerAI) is to build a classification algorithm for identifying if a particular time series input includes an exoplanet or not. It also mentions that a number of methods were tested: 1-D CNN in Torch7, XGBoost in R and PCA in Python. However, none of these methods provided strong results according to the kaggle and Github references. \n",
    "\n",
    "For this project, I would evaluate deep learning algorithms. Per paper in the paragraph (below), these algorithms appear to provide better results compared to the ones already tried as mentioned above.\n",
    "\n",
    "1)\tInitially I would evaluate 1-D CNN using Keras instead of Torch7. \n",
    "2)\tBased on reference paper, I would try adding different number of layers and filters in combination with other CNN parameters. Details would be included in project report.\n",
    "3)\tThe output of the CNNs would be the input to one or more dense layers.\n",
    "4)\tPerformance of each model to be measured as per evaluation metrics section.\n",
    "5)\tPer kaggle source the test set is confirmed to have 5 exoplanets. This will also be useful on checking performance of algorithms. If an algorithm is unable to identify exoplanets on then testing set, then model might not be good. \n",
    "\n",
    "Please that the list above of models considered is not meant to be exhaustive for all possible scenarios in deep learning algorithms. It might be the case that other deep learning algorithms might be considered later should the ones proposed (above) fail in identifying any exoplanet as expected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project design\n",
    "\n",
    "The workflow for solving this problem would have the following order:\n",
    "\n",
    "    1) Exploring the datasets (check dimensions of data, labels, etc.)\n",
    "    2) Data preprocessing: \n",
    "    2.1) The datasets provided by kaggle are supposed to be clean (for the most part).\n",
    "    2.2) It is noted that the data is not normalized. (Some normalization is required.)\n",
    "    2.3) Github reference (above) mentions that techniques like data augmentation could help as we are dealing with time series. Perhaps systematically shifting rows and adding noise could generate additional realistic (albeit synthetic) trends. This might be explored depending on results from the different algorithms tested for solving the problem.\n",
    "    3) Evaluate machine learning algorithms: This involves building the models and selecting best model by using evaluation metrics and comparing to benchmark model(s).\n",
    "    4) Model tuning to optimize results: This involves using evaluating performance of the model and fine tuning hyper-parameters until a satisfactory model is identified. This is by using evaluation metrics to evaluate performance.\n",
    "    5) Final conclusions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import itertools\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "from get_results import plot_roc_auc, confusion_matrix_com\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Some Sklearn libraries are required\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.utils.fixes import signature\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data from Exoplanet dataset\n",
    "train_data = pd.read_csv('kepler/exoTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5087, 3198)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find dimensions of the train data\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing  if train_data has any null fields\n",
    "testing = pd.isnull(train_data)\n",
    "testing *= 1\n",
    "testing2 = testing.sum()\n",
    "testing2.sum() # If results is zero, then there a no fields with \"null\" value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting X_train and y_train\n",
    "# Using iloc to select data using position instead of label and converting to numpy array using values\n",
    "X_train = train_data.iloc[:,1:].values \n",
    "y_train = train_data.iloc[:,0:1].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5087, 3197)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find dimensions of the X_train data\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5087, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find dimensions of the labels (y_train) data\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_train: Label is 2 for exoplanet and 1 for non-exoplanet\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train -= 1 # Changing labels to: 1 for exoplanet and 0 for non-exoplanet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the testing data from Exoplanet dataset\n",
    "test_data = pd.read_csv('kepler/exoTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(570, 3198)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find dimensions of the test data\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing  if test_data has any \"null\" fields\n",
    "testing = pd.isnull(test_data)\n",
    "testing *= 1\n",
    "testing2 = testing.sum()\n",
    "testing2.sum() # If results is zero, then there a no fields with \"null\" value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting X_test and y_test\n",
    "# Using iloc to select data using position instead of label and converting to numpy array (using values)\n",
    "X_test = test_data.iloc[:,1:].values\n",
    "y_test = test_data.iloc[:,0:1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(570, 3197)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find dimensions of the X_test data\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(570, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find dimensions of the labels (y_test) data\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_test: Label is 2 for exoplanet and 1 for non-exoplanet\n",
    "y_test[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test -= 1 # Changing labels to: 1 for exoplanet and 0 for non-exoplanet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data since it is not normalized according to Kaggle/Github\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  93.85,   83.81,   20.1 , ...,   61.42,    5.08,  -39.54],\n",
       "       [ -38.88,  -33.83,  -58.54, ...,    6.46,   16.  ,   19.93],\n",
       "       [ 532.64,  535.92,  513.73, ...,  -28.91,  -70.02,  -96.67],\n",
       "       ..., \n",
       "       [ 273.39,  278.  ,  261.73, ...,   88.42,   79.07,   79.43],\n",
       "       [   3.82,    2.09,   -3.29, ...,  -14.55,   -6.41,   -2.55],\n",
       "       [ 323.28,  306.36,  293.16, ...,  -16.72,  -14.09,   27.82]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking X_train data\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  93.85,  -38.88,  532.64, ...,  273.39,    3.82,  323.28],\n",
       "       [  83.81,  -33.83,  535.92, ...,  278.  ,    2.09,  306.36],\n",
       "       [  20.1 ,  -58.54,  513.73, ...,  261.73,   -3.29,  293.16],\n",
       "       ..., \n",
       "       [  61.42,    6.46,  -28.91, ...,   88.42,  -14.55,  -16.72],\n",
       "       [   5.08,   16.  ,  -70.02, ...,   79.07,   -6.41,  -14.09],\n",
       "       [ -39.54,   19.93,  -96.67, ...,   79.43,   -2.55,   27.82]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tranposing X_train before applying scaling such that mean is zero and variance is one\n",
    "X_train.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.19880000e+02,   1.00210000e+02,   8.64600000e+01, ...,\n",
       "          3.57800000e+01,   2.69430000e+02,   5.77200000e+01],\n",
       "       [  5.73659000e+03,   5.69998000e+03,   5.71716000e+03, ...,\n",
       "         -2.36619000e+03,  -2.29486000e+03,  -2.03472000e+03],\n",
       "       [  8.44480000e+02,   8.17490000e+02,   7.70070000e+02, ...,\n",
       "         -1.62680000e+02,  -3.67900000e+01,   3.06300000e+01],\n",
       "       ..., \n",
       "       [ -5.40100000e+01,  -4.41300000e+01,  -4.12300000e+01, ...,\n",
       "          5.47000000e+00,   1.44600000e+01,   1.87000000e+01],\n",
       "       [  9.13600000e+01,   8.56000000e+01,   4.88100000e+01, ...,\n",
       "         -8.43000000e+00,  -6.48000000e+00,   1.76000000e+01],\n",
       "       [  3.07119000e+03,   2.78253000e+03,   2.60869000e+03, ...,\n",
       "         -2.77220000e+02,  -6.96300000e+01,   1.21560000e+02]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Checking X_train data\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.19880000e+02,   5.73659000e+03,   8.44480000e+02, ...,\n",
       "         -5.40100000e+01,   9.13600000e+01,   3.07119000e+03],\n",
       "       [  1.00210000e+02,   5.69998000e+03,   8.17490000e+02, ...,\n",
       "         -4.41300000e+01,   8.56000000e+01,   2.78253000e+03],\n",
       "       [  8.64600000e+01,   5.71716000e+03,   7.70070000e+02, ...,\n",
       "         -4.12300000e+01,   4.88100000e+01,   2.60869000e+03],\n",
       "       ..., \n",
       "       [  3.57800000e+01,  -2.36619000e+03,  -1.62680000e+02, ...,\n",
       "          5.47000000e+00,  -8.43000000e+00,  -2.77220000e+02],\n",
       "       [  2.69430000e+02,  -2.29486000e+03,  -3.67900000e+01, ...,\n",
       "          1.44600000e+01,  -6.48000000e+00,  -6.96300000e+01],\n",
       "       [  5.77200000e+01,  -2.03472000e+03,   3.06300000e+01, ...,\n",
       "          1.87000000e+01,   1.76000000e+01,   1.21560000e+02]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tranposing X_test before applying scaling such that mean is zero and variance is one\n",
    "X_test.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features by removing the mean and scaling to unit variance (sklearn.preprocessing)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # Output in a numpy.ndarray\n",
    "X_test = scaler.fit_transform(X_test) # Output in a numpy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00235557, -0.00852774,  0.01804893, ...,  0.00599336,\n",
       "        -0.00654212,  0.00831333],\n",
       "       [-0.00205404, -0.0074516 ,  0.01868969, ...,  0.00685579,\n",
       "        -0.00580352,  0.00815701],\n",
       "       [-0.00579778, -0.00938685,  0.01673115, ...,  0.00523005,\n",
       "        -0.00686528,  0.00666449],\n",
       "       ..., \n",
       "       [ 0.0341983 ,  0.03109682,  0.02910084, ...,  0.03572195,\n",
       "         0.0299112 ,  0.02978874],\n",
       "       [ 0.02736753,  0.02803863,  0.02275218, ...,  0.03191466,\n",
       "         0.0266614 ,  0.02618942],\n",
       "       [ 0.01805157,  0.02216476,  0.01410023, ...,  0.02628002,\n",
       "         0.02060995,  0.02271046]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.transpose() # Transpose back to original dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03143654,  0.41497541,  0.02615413, ..., -0.04525719,\n",
       "        -0.03370329,  0.20313139],\n",
       "       [-0.05057432,  0.39343385,  0.00629912, ..., -0.0620191 ,\n",
       "        -0.05173276,  0.16210798],\n",
       "       [-0.03559448,  0.41363759,  0.01894572, ..., -0.04578193,\n",
       "        -0.0385983 ,  0.16563567],\n",
       "       ..., \n",
       "       [-0.00907555, -0.2624662 , -0.03001166, ..., -0.01227304,\n",
       "        -0.01373939, -0.04209481],\n",
       "       [ 0.00470934, -0.26591026, -0.02760726, ..., -0.02219864,\n",
       "        -0.02440852, -0.03107299],\n",
       "       [-0.00786554, -0.22375419, -0.01066057, ..., -0.01189145,\n",
       "        -0.01200494, -0.00127881]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.transpose() # Transpose back to original dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed for reproducibility\n",
    "seed = 10\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing Keras libraries\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv1D, MaxPool1D, Dense, Dropout, Flatten\n",
    "from keras.layers import BatchNormalization, Input, concatenate, Activation\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "from keras.callbacks import ModelCheckpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data into 3d tensor (Input 0 in Conv1D is incompatible with layer conv1d_1: expected ndim=3, found ndim=2)\n",
    "X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))\n",
    "X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(570, 3197, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking shape of X_test tensor\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5087, 3197, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking shape of X_train_new tensor\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration and preparation ended (above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ending Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference on using GridSearch for tuning Hyperparameters for DL models:\n",
    "\n",
    "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH 3cnn, 2dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a subset of the training set\n",
    "X_GS1000 = X_train[0:1000,:]\n",
    "y_GS1000 = y_train[0:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model_drop(dropout_rate=0.3, lr=0.001, beta_1=0.9, beta_2=0.999):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # Defining network architecture\n",
    "    model.add(Conv1D(filters=8, kernel_size=8, activation='relu', input_shape=(3197,1)))\n",
    "    model.add(MaxPool1D(strides=4))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=16, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPool1D(strides=4))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPool1D(strides=4))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    optimizer = Adam(lr=lr, beta_1=beta_1, beta_2=beta_2)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "# Fix random seed for reproducibility (done above on previous cells)\n",
    "# Dataset was created in previous section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = KerasClassifier(build_fn=create_model_drop, epochs=50, batch_size=80, verbose=2)\n",
    "# define the grid search parameters\n",
    "lr = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "beta_1 = [0.8, 0.9, 0.99, 0.999]\n",
    "beta_2 = [0.8, 0.9, 0.99, 0.999]\n",
    "param_grid = dict(lr=lr, beta_1=beta_1, beta_2=beta_2)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_GS1000, y_GS1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.963000 using {'lr': 0.1, 'beta_1': 0.8, 'beta_2': 0.8}\n",
      "0.664000 (0.399593) with: {'lr': 0.001, 'beta_1': 0.8, 'beta_2': 0.8}\n",
      "0.962000 (0.051554) with: {'lr': 0.01, 'beta_1': 0.8, 'beta_2': 0.8}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.8, 'beta_2': 0.8}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.8, 'beta_2': 0.8}\n",
      "0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.8, 'beta_2': 0.8}\n",
      "0.962000 (0.051554) with: {'lr': 0.001, 'beta_1': 0.8, 'beta_2': 0.9}\n",
      "0.962000 (0.051554) with: {'lr': 0.01, 'beta_1': 0.8, 'beta_2': 0.9}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.8, 'beta_2': 0.9}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.8, 'beta_2': 0.9}\n",
      "0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.8, 'beta_2': 0.9}\n",
      "0.962000 (0.051554) with: {'lr': 0.001, 'beta_1': 0.8, 'beta_2': 0.99}\n",
      "0.646000 (0.424914) with: {'lr': 0.01, 'beta_1': 0.8, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.8, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.8, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.8, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.001, 'beta_1': 0.8, 'beta_2': 0.999}\n",
      "0.962000 (0.051554) with: {'lr': 0.01, 'beta_1': 0.8, 'beta_2': 0.999}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.8, 'beta_2': 0.999}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.8, 'beta_2': 0.999}\n",
      "0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.8, 'beta_2': 0.999}\n",
      "0.963000 (0.052248) with: {'lr': 0.001, 'beta_1': 0.9, 'beta_2': 0.8}\n",
      "0.645000 (0.426321) with: {'lr': 0.01, 'beta_1': 0.9, 'beta_2': 0.8}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.9, 'beta_2': 0.8}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.9, 'beta_2': 0.8}\n",
      "0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.9, 'beta_2': 0.8}\n",
      "0.961000 (0.050890) with: {'lr': 0.001, 'beta_1': 0.9, 'beta_2': 0.9}\n",
      "0.962000 (0.051554) with: {'lr': 0.01, 'beta_1': 0.9, 'beta_2': 0.9}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.9, 'beta_2': 0.9}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.9, 'beta_2': 0.9}\n",
      "0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.9, 'beta_2': 0.9}\n",
      "0.962000 (0.051554) with: {'lr': 0.001, 'beta_1': 0.9, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.01, 'beta_1': 0.9, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.9, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.9, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.9, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.001, 'beta_1': 0.9, 'beta_2': 0.999}\n",
      "0.962000 (0.051554) with: {'lr': 0.01, 'beta_1': 0.9, 'beta_2': 0.999}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.9, 'beta_2': 0.999}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.9, 'beta_2': 0.999}\n",
      "0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.9, 'beta_2': 0.999}\n",
      "0.961000 (0.050890) with: {'lr': 0.001, 'beta_1': 0.99, 'beta_2': 0.8}\n",
      "0.963000 (0.052248) with: {'lr': 0.01, 'beta_1': 0.99, 'beta_2': 0.8}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.99, 'beta_2': 0.8}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.99, 'beta_2': 0.8}\n",
      "0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.99, 'beta_2': 0.8}\n",
      "0.963000 (0.052248) with: {'lr': 0.001, 'beta_1': 0.99, 'beta_2': 0.9}\n",
      "0.960000 (0.050258) with: {'lr': 0.01, 'beta_1': 0.99, 'beta_2': 0.9}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.99, 'beta_2': 0.9}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.99, 'beta_2': 0.9}\n",
      "0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.99, 'beta_2': 0.9}\n",
      "0.963000 (0.052248) with: {'lr': 0.001, 'beta_1': 0.99, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.01, 'beta_1': 0.99, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.99, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.99, 'beta_2': 0.99}\n",
      "0.962000 (0.051554) with: {'lr': 0.3, 'beta_1': 0.99, 'beta_2': 0.99}\n",
      "0.962000 (0.051554) with: {'lr': 0.001, 'beta_1': 0.99, 'beta_2': 0.999}\n",
      "0.963000 (0.052248) with: {'lr': 0.01, 'beta_1': 0.99, 'beta_2': 0.999}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.99, 'beta_2': 0.999}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.99, 'beta_2': 0.999}\n",
      "0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.99, 'beta_2': 0.999}\n",
      "0.961000 (0.050890) with: {'lr': 0.001, 'beta_1': 0.999, 'beta_2': 0.8}\n",
      "0.961000 (0.050890) with: {'lr': 0.01, 'beta_1': 0.999, 'beta_2': 0.8}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.999, 'beta_2': 0.8}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.999, 'beta_2': 0.8}\n",
      "0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.999, 'beta_2': 0.8}\n",
      "0.963000 (0.052248) with: {'lr': 0.001, 'beta_1': 0.999, 'beta_2': 0.9}\n",
      "0.963000 (0.052248) with: {'lr': 0.01, 'beta_1': 0.999, 'beta_2': 0.9}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.999, 'beta_2': 0.9}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.999, 'beta_2': 0.9}\n",
      "0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.999, 'beta_2': 0.9}\n",
      "0.963000 (0.052248) with: {'lr': 0.001, 'beta_1': 0.999, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.01, 'beta_1': 0.999, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.999, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.999, 'beta_2': 0.99}\n",
      "0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.999, 'beta_2': 0.99}\n",
      "0.961000 (0.050890) with: {'lr': 0.001, 'beta_1': 0.999, 'beta_2': 0.999}\n",
      "0.963000 (0.052248) with: {'lr': 0.01, 'beta_1': 0.999, 'beta_2': 0.999}\n",
      "0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.999, 'beta_2': 0.999}\n",
      "0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.999, 'beta_2': 0.999}\n",
      "0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.999, 'beta_2': 0.999}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best: 0.963000 using {'lr': 0.1, 'beta_1': 0.8, 'beta_2': 0.8}\n",
    "    0.664000 (0.399593) with: {'lr': 0.001, 'beta_1': 0.8, 'beta_2': 0.8}\n",
    "    0.962000 (0.051554) with: {'lr': 0.01, 'beta_1': 0.8, 'beta_2': 0.8}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.8, 'beta_2': 0.8}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.8, 'beta_2': 0.8}\n",
    "    0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.8, 'beta_2': 0.8}\n",
    "    0.962000 (0.051554) with: {'lr': 0.001, 'beta_1': 0.8, 'beta_2': 0.9}\n",
    "    0.962000 (0.051554) with: {'lr': 0.01, 'beta_1': 0.8, 'beta_2': 0.9}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.8, 'beta_2': 0.9}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.8, 'beta_2': 0.9}\n",
    "    0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.8, 'beta_2': 0.9}\n",
    "    0.962000 (0.051554) with: {'lr': 0.001, 'beta_1': 0.8, 'beta_2': 0.99}\n",
    "    0.646000 (0.424914) with: {'lr': 0.01, 'beta_1': 0.8, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.8, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.8, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.8, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.001, 'beta_1': 0.8, 'beta_2': 0.999}\n",
    "    0.962000 (0.051554) with: {'lr': 0.01, 'beta_1': 0.8, 'beta_2': 0.999}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.8, 'beta_2': 0.999}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.8, 'beta_2': 0.999}\n",
    "    0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.8, 'beta_2': 0.999}\n",
    "    0.963000 (0.052248) with: {'lr': 0.001, 'beta_1': 0.9, 'beta_2': 0.8}\n",
    "    0.645000 (0.426321) with: {'lr': 0.01, 'beta_1': 0.9, 'beta_2': 0.8}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.9, 'beta_2': 0.8}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.9, 'beta_2': 0.8}\n",
    "    0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.9, 'beta_2': 0.8}\n",
    "    0.961000 (0.050890) with: {'lr': 0.001, 'beta_1': 0.9, 'beta_2': 0.9}\n",
    "    0.962000 (0.051554) with: {'lr': 0.01, 'beta_1': 0.9, 'beta_2': 0.9}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.9, 'beta_2': 0.9}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.9, 'beta_2': 0.9}\n",
    "    0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.9, 'beta_2': 0.9}\n",
    "    0.962000 (0.051554) with: {'lr': 0.001, 'beta_1': 0.9, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.01, 'beta_1': 0.9, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.9, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.9, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.9, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.001, 'beta_1': 0.9, 'beta_2': 0.999}\n",
    "    0.962000 (0.051554) with: {'lr': 0.01, 'beta_1': 0.9, 'beta_2': 0.999}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.9, 'beta_2': 0.999}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.9, 'beta_2': 0.999}\n",
    "    0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.9, 'beta_2': 0.999}\n",
    "    0.961000 (0.050890) with: {'lr': 0.001, 'beta_1': 0.99, 'beta_2': 0.8}\n",
    "    0.963000 (0.052248) with: {'lr': 0.01, 'beta_1': 0.99, 'beta_2': 0.8}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.99, 'beta_2': 0.8}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.99, 'beta_2': 0.8}\n",
    "    0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.99, 'beta_2': 0.8}\n",
    "    0.963000 (0.052248) with: {'lr': 0.001, 'beta_1': 0.99, 'beta_2': 0.9}\n",
    "    0.960000 (0.050258) with: {'lr': 0.01, 'beta_1': 0.99, 'beta_2': 0.9}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.99, 'beta_2': 0.9}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.99, 'beta_2': 0.9}\n",
    "    0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.99, 'beta_2': 0.9}\n",
    "    0.963000 (0.052248) with: {'lr': 0.001, 'beta_1': 0.99, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.01, 'beta_1': 0.99, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.99, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.99, 'beta_2': 0.99}\n",
    "    0.962000 (0.051554) with: {'lr': 0.3, 'beta_1': 0.99, 'beta_2': 0.99}\n",
    "    0.962000 (0.051554) with: {'lr': 0.001, 'beta_1': 0.99, 'beta_2': 0.999}\n",
    "    0.963000 (0.052248) with: {'lr': 0.01, 'beta_1': 0.99, 'beta_2': 0.999}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.99, 'beta_2': 0.999}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.99, 'beta_2': 0.999}\n",
    "    0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.99, 'beta_2': 0.999}\n",
    "    0.961000 (0.050890) with: {'lr': 0.001, 'beta_1': 0.999, 'beta_2': 0.8}\n",
    "    0.961000 (0.050890) with: {'lr': 0.01, 'beta_1': 0.999, 'beta_2': 0.8}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.999, 'beta_2': 0.8}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.999, 'beta_2': 0.8}\n",
    "    0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.999, 'beta_2': 0.8}\n",
    "    0.963000 (0.052248) with: {'lr': 0.001, 'beta_1': 0.999, 'beta_2': 0.9}\n",
    "    0.963000 (0.052248) with: {'lr': 0.01, 'beta_1': 0.999, 'beta_2': 0.9}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.999, 'beta_2': 0.9}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.999, 'beta_2': 0.9}\n",
    "    0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.999, 'beta_2': 0.9}\n",
    "    0.963000 (0.052248) with: {'lr': 0.001, 'beta_1': 0.999, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.01, 'beta_1': 0.999, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.999, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.999, 'beta_2': 0.99}\n",
    "    0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.999, 'beta_2': 0.99}\n",
    "    0.961000 (0.050890) with: {'lr': 0.001, 'beta_1': 0.999, 'beta_2': 0.999}\n",
    "    0.963000 (0.052248) with: {'lr': 0.01, 'beta_1': 0.999, 'beta_2': 0.999}\n",
    "    0.963000 (0.052248) with: {'lr': 0.1, 'beta_1': 0.999, 'beta_2': 0.999}\n",
    "    0.963000 (0.052248) with: {'lr': 0.2, 'beta_1': 0.999, 'beta_2': 0.999}\n",
    "    0.963000 (0.052248) with: {'lr': 0.3, 'beta_1': 0.999, 'beta_2': 0.999}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
